"""
Result analysis and summary generation utilities.
"""

from typing import List, Dict, Any


def generate_summary(results: List[Dict[str, Any]], output_file: str) -> str:
    """
    Generuje podsumowanie wynikÃ³w testÃ³w.
    
    Args:
        results (List[Dict[str, Any]]): Lista wynikÃ³w testÃ³w
        output_file (str): Nazwa pliku wyjÅ›ciowego
        
    Returns:
        str: Sformatowane podsumowanie
    """
    if not results:
        return ""
    
    summary = f"\n{'='*100}\n"
    summary += "PODSUMOWANIE WYNIKÃ“W\n"
    summary += f"{'='*100}\n\n"
    
    models = list(set(r['model'] for r in results))
    
    summary += "Åšrednie czasy odpowiedzi i oceny jakoÅ›ci:\n"
    summary += "-" * 60 + "\n"
    
    for model in models:
        model_results = [r for r in results if r['model'] == model]
        if model_results:
            avg_first_token = sum(r['first_token_time'] for r in model_results) / len(model_results)
            avg_total_time = sum(r['total_time'] for r in model_results) / len(model_results)
            avg_length = sum(r['response_length'] for r in model_results) / len(model_results)
            
            # Oblicz Å›redniÄ… ocenÄ™ sÄ™dziego, jeÅ›li dostÄ™pne
            judge_ratings = [r['judge_rating'] for r in model_results if 'judge_rating' in r and r['judge_rating'] > 0]
            avg_judge_rating = sum(judge_ratings) / len(judge_ratings) if judge_ratings else "N/A"
            
            summary += f"{model}:\n"
            summary += f"  - Åšredni czas pierwszego tokena: {avg_first_token:.2f}s\n"
            summary += f"  - Åšredni caÅ‚kowity czas: {avg_total_time:.2f}s\n"
            summary += f"  - Åšrednia dÅ‚ugoÅ›Ä‡ odpowiedzi: {avg_length:.0f} znakÃ³w\n"
            summary += f"  - ZakoÅ„czonych testÃ³w: {len(model_results)}\n"
            summary += f"  - Åšrednia ocena sÄ™dziego AI: {avg_judge_rating:.2f}/5\n" if isinstance(avg_judge_rating, float) else f"  - Åšrednia ocena sÄ™dziego AI: {avg_judge_rating}\n"
            summary += "\n"
    
    summary += "ðŸ† RANKING SZYBKOÅšCI (pierwszy token - niÅ¼ej = lepiej):\n"
    summary += "-" * 60 + "\n"
    
    model_speed = {}
    for model in models:
        model_results = [r for r in results if r['model'] == model]
        if model_results:
            model_speed[model] = sum(r['first_token_time'] for r in model_results) / len(model_results)
    
    sorted_models_speed = sorted(model_speed.items(), key=lambda x: x[1])
    for i, (model, speed) in enumerate(sorted_models_speed, 1):
        medal = "ðŸ¥‡" if i == 1 else "ðŸ¥ˆ" if i == 2 else "ðŸ¥‰" if i == 3 else "  "
        summary += f"{medal} {i}. {model}: {speed:.2f}s\n"

    summary += f"\nðŸŽ–ï¸ RANKING JAKOÅšCI (ocena sÄ™dziego AI - wyÅ¼ej = lepiej):\n"
    summary += "-" * 60 + "\n"

    model_quality = {}
    for model in models:
        model_results = [r for r in results if r['model'] == model]
        if model_results:
            judge_ratings = [r['judge_rating'] for r in model_results if 'judge_rating' in r and r['judge_rating'] > 0]
            if judge_ratings:
                model_quality[model] = sum(judge_ratings) / len(judge_ratings)
    
    if model_quality:
        sorted_models_quality = sorted(model_quality.items(), key=lambda x: x[1], reverse=True)
        for i, (model, quality) in enumerate(sorted_models_quality, 1):
            summary += f"{i}. {model}: {quality:.2f}/5\n"
    else:
        summary += "Brak danych o ocenach sÄ™dziego AI (upewnij siÄ™, Å¼e klucz API Gemini jest poprawny).\n"
    
    # Zapisz podsumowanie do pliku
    with open(output_file, 'a', encoding='utf-8') as f:
        f.write(summary)
    
    return summary
