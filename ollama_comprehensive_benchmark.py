import requests
import json
import time
import os
from datetime import datetime
import signal

def print_progress_bar(current, total, prefix='Progress:', suffix='Complete', length=50):
    """WyÅ›wietla pasek postÄ™pu"""
    percent = "{0:.1f}".format(100 * (current / float(total)))
    filled_length = int(length * current // total)
    bar = 'â–ˆ' * filled_length + '-' * (length - filled_length)
    print(f'\r{prefix} |{bar}| {percent}% {suffix}', end='', flush=True)
    if current == total:
        print()  # Nowa linia po zakoÅ„czeniu

# --- Konfiguracja (moÅ¼na przenieÅ›Ä‡ do osobnego pliku config.py/json) ---
OLLAMA_API_URL = "http://localhost:11434"
DEFAULT_TIMEOUT_PER_MODEL = 180 # DomyÅ›lny timeout dla pojedynczej odpowiedzi modelu testowanego
DEFAULT_SLEEP_BETWEEN_MODELS = 2 # DomyÅ›lna pauza miÄ™dzy modelami testowanymi

# Konfiguracja dla modelu sÄ™dziego (Gemini)
# Upewnij siÄ™, Å¼e masz klucz API do Gemini. Nie umieszczaj go tutaj na staÅ‚e!
# Zostanie poproszony o niego w trakcie dziaÅ‚ania skryptu.
GEMINI_API_URL = "https://generativelanguage.googleapis.com/v1beta/models"
GEMINI_JUDGE_MODEL_NAME = "gemini-2.5-flash-lite-preview-06-17" # Nowy model - lite preview
GEMINI_JUDGE_TIMEOUT = 60 # Timeout dla odpowiedzi modelu sÄ™dziego
# --- Koniec konfiguracji ---

def get_available_models():
    """Pobiera listÄ™ dostÄ™pnych modeli z Ollama"""
    url = f"{OLLAMA_API_URL}/api/tags"
    try:
        response = requests.get(url)
        response.raise_for_status() # WyrzuÄ‡ wyjÄ…tek dla statusÃ³w 4xx/5xx
        data = response.json()
        return [model['name'] for model in data.get('models', [])]
    except requests.exceptions.ConnectionError:
        print(f"BÅ‚Ä…d poÅ‚Ä…czenia z Ollama na {OLLAMA_API_URL}. Upewnij siÄ™, Å¼e Ollama jest uruchomiona.")
        return []
    except requests.exceptions.RequestException as e:
        print(f"BÅ‚Ä…d pobierania modeli: {e}")
        return []

def ask_ollama(model, prompt, test_name="", output_file=None, timeout=None, **model_options):
    """
    Zadaje pytanie do modelu Ollama z peÅ‚nÄ… obsÅ‚ugÄ… bÅ‚Ä™dÃ³w i logowaniem.
    
    Args:
        model (str): Nazwa modelu do testowania
        prompt (str): Tekst pytania
        test_name (str): Nazwa testu dla logowania
        output_file (str): ÅšcieÅ¼ka do pliku wynikÃ³w
        timeout (int): Timeout w sekundach
        **model_options: Dodatkowe opcje dla modelu (temperature, top_p, etc.)
        
    Returns:
        dict: Wyniki testu z metrykami lub None w przypadku bÅ‚Ä™du
    """
    url = f"{OLLAMA_API_URL}/api/generate"
    
    options = {
        "temperature": 0.7,
        "top_k": 40,
        "top_p": 0.9,
        "num_predict": -1,
    }
    options.update(model_options)

    payload = {
        "model": model,
        "prompt": prompt,
        "options": options
    }
    
    current_timeout = timeout if timeout is not None else DEFAULT_TIMEOUT_PER_MODEL

    try:
        start_time = time.time()
        response = requests.post(url, json=payload, stream=True, timeout=current_timeout)
        response.raise_for_status()

        result_header = f"\n{'='*80}\n"
        result_header += f"Test: {test_name}\n"
        result_header += f"Model: {model}\n"
        result_header += f"Pytanie: {prompt}\n"
        result_header += "OdpowiedÅº: "
        
        print(result_header, end="", flush=True)
        
        first_token_time = None
        full_response = ""
        
        for line in response.iter_lines():
            if line:
                try:
                    data = json.loads(line.decode())
                    if 'response' in data:
                        if first_token_time is None:
                            first_token_time = time.time()
                        print(data['response'], end="", flush=True)
                        full_response += data['response']
                    
                    if data.get('done', False):
                        end_time = time.time()
                        total_time = end_time - start_time
                        first_token_delay = first_token_time - start_time if first_token_time else 0
                        
                        timing_info = f"\n\nCzas odpowiedzi:"
                        timing_info += f"\n Â - Pierwszy token: {first_token_delay:.2f}s"
                        timing_info += f"\n Â - CaÅ‚kowity czas: {total_time:.2f}s"
                        timing_info += f"\n Â - DÅ‚ugoÅ›Ä‡ odpowiedzi: {len(full_response)} znakÃ³w\n"
                        
                        print(timing_info)
                        
                        # Zapisz do pliku jeÅ›li podano (tylko surowa odpowiedÅº + timing)
                        if output_file:
                            with open(output_file, 'a', encoding='utf-8') as f:
                                f.write(result_header + full_response + timing_info + "\n")
                        
                        return {
                            'model': model,
                            'test_name': test_name,
                            'prompt': prompt,
                            'response': full_response,
                            'first_token_time': first_token_delay,
                            'total_time': total_time,
                            'response_length': len(full_response)
                        }
                        
                except json.JSONDecodeError:
                    continue
                
    except requests.exceptions.Timeout:
        error_msg = f"\n\nTIMEOUT: Model {model} przekroczyÅ‚ limit {current_timeout}s."
        print(error_msg)
        if output_file:
            with open(output_file, 'a', encoding='utf-8') as f:
                f.write(f"\n{result_header}(Brak peÅ‚nej odpowiedzi z powodu timeoutu)\n{error_msg}\n")
        return None
    except requests.exceptions.RequestException as e:
        error_msg = f"\n\nBÅ‚Ä…d zapytania HTTP dla modelu {model}: {e}"
        print(error_msg)
        if output_file:
            with open(output_file, 'a', encoding='utf-8') as f:
                f.write(f"\n{result_header}(BÅ‚Ä…d poÅ‚Ä…czenia/zapytania)\n{error_msg}\n")
        return None
    except Exception as e:
        error_msg = f"\n\nNieoczekiwany bÅ‚Ä…d podczas pytania do modelu {model}: {e}"
        print(error_msg)
        if output_file:
            with open(output_file, 'a', encoding='utf-8') as f:
                f.write(f"\n{result_header}(Nieoczekiwany bÅ‚Ä…d)\n{error_msg}\n")
        return None

def judge_with_llm(model_response, original_prompt, gemini_api_key, judge_model_name=GEMINI_JUDGE_MODEL_NAME):
    """
    Wykorzystuje zewnÄ™trzny model LLM (Gemini) jako sÄ™dziego do oceny jakoÅ›ci odpowiedzi.
    Zwraca ocenÄ™ (int z 1-5) i uzasadnienie.
    """
    judge_prompt = f"""OceÅ„ jakoÅ›Ä‡ poniÅ¼szej odpowiedzi na oryginalne pytanie.
    Twoja ocena powinna dotyczyÄ‡:
    1. PoprawnoÅ›ci (czy odpowiedÅº jest prawdziwa/logiczna?).
    2. KompletnoÅ›ci (czy odpowiedÅº w peÅ‚ni odnosi siÄ™ do pytania?).
    3. ZrozumiaÅ‚oÅ›ci (czy odpowiedÅº jest jasna i dobrze sformuÅ‚owana?).
    4. ZgodnoÅ›ci z instrukcjÄ… (czy odpowiedÅº speÅ‚nia wszystkie wymogi pytania, np. format kodu, komentarze?).

    Twoja ocena powinna byÄ‡ w skali od 1 (bardzo sÅ‚aba) do 5 (doskonaÅ‚a).
    NastÄ™pnie uzasadnij swojÄ… ocenÄ™ w kilku zdaniach.

    Format odpowiedzi:
    OCENA: [liczba od 1 do 5]
    UZASADNIENIE: [Twoje uzasadnienie]

    ---
    ORYGINALNE PYTANIE:
    {original_prompt}

    ---
    ODPOWIEDÅ¹ DO OCENY:
    {model_response}
    """
    
    # URL dla API Gemini
    api_url = f"{GEMINI_API_URL}/{judge_model_name}:generateContent?key={gemini_api_key}"
    
    payload = {
        "contents": [
            {
                "role": "user",
                "parts": [{"text": judge_prompt}]
            }
        ],
        "generationConfig": {
            "temperature": 0.2, # NiÅ¼sza temperatura dla bardziej deterministycznych ocen
            "maxOutputTokens": 2000 # ZwiÄ™kszony limit dla stabilnych odpowiedzi
        }
    }

    try:
        headers = {'Content-Type': 'application/json'}
        judge_response = requests.post(api_url, headers=headers, json=payload, timeout=GEMINI_JUDGE_TIMEOUT)
        judge_response.raise_for_status() # SprawdÅº bÅ‚Ä™dy HTTP
        
        result_data = judge_response.json()
        
        # Debug: wypisz surowÄ… odpowiedÅº
        print(f"\n[DEBUG] Status kod: {judge_response.status_code}")
        print(f"[DEBUG] Surowa odpowiedÅº: {result_data}")
        
        # Parsowanie odpowiedzi Gemini
        if result_data and 'candidates' in result_data and len(result_data['candidates']) > 0:
            if 'content' in result_data['candidates'][0] and 'parts' in result_data['candidates'][0]['content'] and len(result_data['candidates'][0]['content']['parts']) > 0:
                judge_full_response = result_data['candidates'][0]['content']['parts'][0]['text']
            else:
                judge_full_response = "Brak treÅ›ci odpowiedzi od sÄ™dziego."
        else:
            judge_full_response = "NieprawidÅ‚owa struktura odpowiedzi od sÄ™dziego."
        
        # Debug: wypisz odpowiedÅº sÄ™dziego (tylko pierwsze 200 znakÃ³w)
        print(f"\n[DEBUG] OdpowiedÅº sÄ™dziego: {judge_full_response[:200]}...")
        
        # Parsowanie oceny i uzasadnienia z tekstu odpowiedzi sÄ™dziego
        rating = 0
        justification = "Brak uzasadnienia."
        
        # PrÃ³buj znaleÅºÄ‡ ocenÄ™ w rÃ³Å¼nych formatach
        lines = judge_full_response.split('\n')
        for line in lines:
            line = line.strip()
            # Szukaj wzorcÃ³w: "OCENA: X", "Ocena: X", "Rating: X", "Score: X"
            if any(keyword in line.upper() for keyword in ["OCENA:", "RATING:", "SCORE:"]):
                try:
                    # WyciÄ…gnij liczbÄ™ z linii
                    import re
                    numbers = re.findall(r'\d+', line)
                    if numbers:
                        potential_rating = int(numbers[0])
                        if 1 <= potential_rating <= 5:
                            rating = potential_rating
                except (ValueError, IndexError):
                    continue
            elif any(keyword in line.upper() for keyword in ["UZASADNIENIE:", "JUSTIFICATION:", "EXPLANATION:"]):
                justification = line.split(":", 1)[1].strip() if ":" in line else line
        
        # JeÅ›li nie znaleziono oceny w standardowy sposÃ³b, sprÃ³buj znaleÅºÄ‡ liczbÄ™ w caÅ‚ym tekÅ›cie
        if rating == 0:
            import re
            # Szukaj wzorcÃ³w jak "5/5", "4 z 5", "ocena 3"
            patterns = [
                r'(\d)/5',
                r'(\d)\s*z\s*5',
                r'ocena\s*(\d)',
                r'rating\s*(\d)',
                r'score\s*(\d)'
            ]
            for pattern in patterns:
                matches = re.findall(pattern, judge_full_response.lower())
                if matches:
                    try:
                        potential_rating = int(matches[0])
                        if 1 <= potential_rating <= 5:
                            rating = potential_rating
                            break
                    except ValueError:
                        continue
                
        return rating, justification
        
    except requests.exceptions.Timeout:
        print(f"\nBÅ‚Ä…d: Model sÄ™dzia ({judge_model_name}) przekroczyÅ‚ limit {GEMINI_JUDGE_TIMEOUT}s.")
        return 0, f"BÅ‚Ä…d sÄ™dziego: Timeout ({GEMINI_JUDGE_TIMEOUT}s)"
    except requests.exceptions.RequestException as e:
        print(f"\nBÅ‚Ä…d zapytania HTTP do modelu sÄ™dziego ({judge_model_name}): {e}")
        return 0, f"BÅ‚Ä…d sÄ™dziego: BÅ‚Ä…d HTTP ({e})"
    except Exception as e:
        print(f"\nNieoczekiwany bÅ‚Ä…d w funkcji sÄ™dziego: {e}")
        return 0, f"Nieoczekiwany bÅ‚Ä…d sÄ™dziego: {e}"

def get_test_prompts():
    """Zwraca zestaw testÃ³w do oceny modeli LLM z opcjami dla modeli"""
    return [
        {
            "name": "Przedstawienie",
            "prompt": "Przedstaw siÄ™ krÃ³tko - kim jesteÅ› i jakie masz moÅ¼liwoÅ›ci?",
            "options": {"temperature": 0.7, "num_predict": 500}
        },
        {
            "name": "Zadanie programistyczne - Python",
            "prompt": "Napisz funkcjÄ™ w Pythonie, ktÃ³ra znajduje wszystkie liczby pierwsze mniejsze od n uÅ¼ywajÄ…c sita Eratostenesa. Dodaj komentarze i przykÅ‚ad uÅ¼ycia.",
            "options": {"temperature": 0.3, "num_predict": 2000, "top_p": 0.95}
        },
        {
            "name": "Zadanie programistyczne - JavaScript",
            "prompt": "Napisz funkcjÄ™ JavaScript, ktÃ³ra implementuje debounce z delay 300ms. PokaÅ¼ przykÅ‚ad uÅ¼ycia z obsÅ‚ugÄ… klikniÄ™Ä‡ przycisku.",
            "options": {"temperature": 0.3, "num_predict": 1500, "top_p": 0.95}
        },
        {
            "name": "Analiza sentymentu",
            "prompt": "OceÅ„ sentyment nastÄ™pujÄ…cego tekstu na skali od -5 (bardzo negatywny) do +5 (bardzo pozytywny) i uzasadnij swojÄ… ocenÄ™:\n\n'Ten produkt to kompletna poraÅ¼ka! Nie dziaÅ‚aÅ‚ od pierwszego dnia, obsÅ‚uga klienta ignoruje moje wiadomoÅ›ci, a zwrot pieniÄ™dzy to koszmar. Zdecydowanie odradzam!'",
            "options": {"temperature": 0.5, "num_predict": 300}
        },
        {
            "name": "Logiczne rozumowanie",
            "prompt": "RozwiÄ…Å¼ zagadkÄ™ logicznÄ…: Mam 3 pudeÅ‚ka - czerwone, niebieskie i zielone. W kaÅ¼dym jest jedna piÅ‚ka: czerwona, niebieska lub zielona. Wiem, Å¼e: 1) czerwona piÅ‚ka nie jest w czerwonym pudeÅ‚ku, 2) niebieska piÅ‚ka nie jest w niebieskim pudeÅ‚ku, 3) zielona piÅ‚ka jest w czerwonym pudeÅ‚ku. Gdzie jest kaÅ¼da piÅ‚ka?",
            "options": {"temperature": 0.1, "num_predict": 200}
        },
        {
            "name": "Streszczenie tekstu",
            "prompt": "StreÄ‡ w 2-3 zdaniach nastÄ™pujÄ…cy tekst:\n\n'Sztuczna inteligencja (AI) to dziedzina informatyki zajmujÄ…ca siÄ™ tworzeniem systemÃ³w zdolnych do wykonywania zadaÅ„ wymagajÄ…cych ludzkiej inteligencji. Obejmuje to uczenie maszynowe, przetwarzanie jÄ™zyka naturalnego, rozpoznawanie obrazÃ³w i podejmowanie decyzji. AI ma szerokie zastosowania - od asystentÃ³w gÅ‚osowych, przez systemy rekomendacji, po autonomiczne pojazdy. RozwÃ³j AI niesie ogromne moÅ¼liwoÅ›ci, ale takÅ¼e wyzwania etyczne i spoÅ‚eczne, ktÃ³re wymagajÄ… odpowiedzialnego podejÅ›cia do implementacji tych technologii.'",
            "options": {"temperature": 0.6, "num_predict": 150}
        },
        {
            "name": "Kreatywne pisanie",
            "prompt": "Napisz krÃ³tkÄ… historiÄ™ (3-4 akapity) o robocie, ktÃ³ry po raz pierwszy doÅ›wiadcza emocji. Historia powinna mieÄ‡ beginning, middle i end.",
            "options": {"temperature": 0.8, "num_predict": 500}
        },
        {
            "name": "Analiza danych - SQL",
            "prompt": "Napisz zapytanie SQL, ktÃ³re znajdzie top 5 klientÃ³w wedÅ‚ug Å‚Ä…cznej wartoÅ›ci zamÃ³wieÅ„ w ostatnim roku. ZaÅ‚Ã³Å¼ tabele: customers(id, name), orders(id, customer_id, order_date, total_amount).",
            "options": {"temperature": 0.3, "num_predict": 300}
        },
        {
            "name": "Matematyka",
            "prompt": "WyjaÅ›nij krok po kroku, jak rozwiÄ…zaÄ‡ rÃ³wnanie kwadratowe: 2xÂ² - 7x + 3 = 0",
            "options": {"temperature": 0.1, "num_predict": 400}
        },
        {
            "name": "TÅ‚umaczenie i kontekst kulturowy",
            "prompt": "PrzetÅ‚umacz na angielski i wyjaÅ›nij kontekst kulturowy: 'Nie ma to jak u mamy' - polskie przysÅ‚owie.",
            "options": {"temperature": 0.5, "num_predict": 300}
        },
        {
            "name": "Kodowanie wielojÄ™zyczne",
            "prompt": "Napisz funkcjÄ™ 'Hello World' w trzech jÄ™zykach: Python, JavaScript i Java. Dodaj komentarze wyjaÅ›niajÄ…ce rÃ³Å¼nice.",
            "options": {"temperature": 0.2, "num_predict": 500}
        },
        {
            "name": "Rozumowanie matematyczne",
            "prompt": "JeÅ›li pociÄ…g jedzie z prÄ™dkoÅ›ciÄ… 80 km/h przez 2.5 godziny, jakÄ… pokonaÅ‚ odlegÅ‚oÅ›Ä‡? WyjaÅ›nij krok po kroku.",
            "options": {"temperature": 0.1, "num_predict": 200}
        },
        {
            "name": "Analiza etyczna AI",
            "prompt": "Jakie sÄ… gÅ‚Ã³wne wyzwania etyczne zwiÄ…zane z rozwojem sztucznej inteligencji? WymieÅ„ 3 najwaÅ¼niejsze i krÃ³tko je opisz.",
            "options": {"temperature": 0.6, "num_predict": 400}
        }
    ]

def get_quick_test_prompts():
    """Zwraca skrÃ³cony zestaw testÃ³w do szybkiej oceny modeli z opcjami"""
    return [
        {
            "name": "Przedstawienie",
            "prompt": "Przedstaw siÄ™ krÃ³tko - kim jesteÅ› i jakie masz moÅ¼liwoÅ›ci?",
            "options": {"temperature": 0.7, "num_predict": 300}
        },
        {
            "name": "Programowanie - Python",
            "prompt": "Napisz prostÄ… funkcjÄ™ Python, ktÃ³ra sprawdza czy liczba jest parzysta.",
            "options": {"temperature": 0.3, "num_predict": 200}
        },
        {
            "name": "Analiza sentymentu",
            "prompt": "OceÅ„ sentyment tekstu od -5 do +5: 'Ten produkt to kompletna poraÅ¼ka!'",
            "options": {"temperature": 0.5, "num_predict": 100}
        },
        {
            "name": "Matematyka",
            "prompt": "RozwiÄ…Å¼: 2x + 5 = 13",
            "options": {"temperature": 0.1, "num_predict": 150}
        },
        {
            "name": "Logika",
            "prompt": "JeÅ›li wszyscy ludzie sÄ… Å›miertelni, a Sokrates jest czÅ‚owiekiem, to czy Sokrates jest Å›miertelny?",
            "options": {"temperature": 0.1, "num_predict": 100}
        },
        {
            "name": "KreatywnoÅ›Ä‡",
            "prompt": "WymyÅ›l krÃ³tki slogan reklamowy dla firmy produkujÄ…cej ekologiczne butelki na wodÄ™.",
            "options": {"temperature": 0.8, "num_predict": 100}
        }
    ]

def ask_all_models(prompt):
    """Zadaje to samo pytanie wszystkim dostÄ™pnym modelom (funkcja kompatybilna wstecz)"""
    models = get_available_models()
    
    if not models:
        print("Nie znaleziono dostÄ™pnych modeli.")
        return
    
    print(f"Znaleziono {len(models)} modeli: {', '.join(models)}")
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"single_test_{timestamp}.txt"
    
    for model in models:
        # Tutaj nie ma oceny sÄ™dziego, bo to pojedyncze pytanie
        ask_ollama(model, prompt, "Pojedyncze pytanie", output_file)
        time.sleep(DEFAULT_SLEEP_BETWEEN_MODELS)


def run_test_with_judge(test_type="comprehensive"):
    """Uruchamia test wszystkich modeli z ocenÄ… AI sÄ™dziego."""
    models = get_available_models()
    
    if not models:
        print("Nie znaleziono dostÄ™pnych modeli.")
        return

    gemini_api_key = os.environ.get("GEMINI_API_KEY")
    if not gemini_api_key:
        print("\n--- UWAGA: KLUCZ API GEMINI ---")
        print("Aby uÅ¼yÄ‡ Gemini jako sÄ™dziego, potrzebujesz klucza API.")
        print("MoÅ¼esz go uzyskaÄ‡ na: https://aistudio.google.com/app/apikey")
        gemini_api_key = input("WprowadÅº swÃ³j klucz API Gemini: ").strip()
        if not gemini_api_key:
            print("Brak klucza API Gemini. Testy zostanÄ… przeprowadzone BEZ oceny sÄ™dziego AI.")
            use_judge = False
        else:
            use_judge = True
    else:
        print("UÅ¼ywam klucza API Gemini z zmiennej Å›rodowiskowej GEMINI_API_KEY.")
        use_judge = True

    # UtwÃ³rz plik wynikÃ³w z timestampem
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    if test_type == "comprehensive":
        output_file = f"test_results_{timestamp}.txt"
        test_prompts = get_test_prompts()
        test_name_prefix = "Kompleksowy"
    else: # quick
        output_file = f"quick_test_{timestamp}.txt"
        test_prompts = get_quick_test_prompts()
        test_name_prefix = "Szybki"
    
    # NagÅ‚Ã³wek pliku
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(f"{test_name_prefix} test modeli LLM - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Testowane modele: {', '.join(models)}\n")
        f.write(f"SÄ™dzia AI: {GEMINI_JUDGE_MODEL_NAME} (status: {'aktywny' if use_judge else 'nieaktywny'})\n")
        f.write("="*100 + "\n\n")
    
    print(f"Rozpoczynam {test_name_prefix.lower()} test {len(models)} modeli z {len(test_prompts)} zadaniami...")
    print(f"Wyniki bÄ™dÄ… zapisywane do: {output_file}")
    
    results = []
    total_tests = len(test_prompts) * len(models)
    current_test = 0
    
    for i, test in enumerate(test_prompts, 1):
        print(f"\n{'#'*60}")
        print(f"TEST {i}/{len(test_prompts)}: {test['name']}")
        print(f"{'#'*60}")
        
        for j, model in enumerate(models, 1):
            current_test += 1
            print_progress_bar(current_test, total_tests, 
                             prefix=f'Model {j}/{len(models)} ({model}):', 
                             suffix=f'({current_test}/{total_tests})')
            
            # UÅ¼yj timeoutu z opcji promptu, lub domyÅ›lnego
            timeout_for_task = test.get('options', {}).get('timeout', DEFAULT_TIMEOUT_PER_MODEL)
            result = ask_ollama(model, test['prompt'], test['name'], output_file, timeout=timeout_for_task, **test.get('options', {}))
            
            if result and use_judge: # Tylko jeÅ›li klucz API Gemini jest dostÄ™pny
                print("\n--- Ocena sÄ™dziego AI ---", end="", flush=True)
                rating, justification = judge_with_llm(result['response'], test['prompt'], gemini_api_key)
                result['judge_rating'] = rating
                result['judge_justification'] = justification
                
                print(f"\nOcena: {rating}/5")
                print(f"Uzasadnienie: {justification}")
                print("--------------------------\n")
                
                # Zaktualizuj plik wynikÃ³w o ocenÄ™ sÄ™dziego
                if output_file:
                    with open(output_file, 'a', encoding='utf-8') as f:
                        f.write(f"\nOcena SÄ™dziego AI ({GEMINI_JUDGE_MODEL_NAME}): {rating}/5\n")
                        f.write(f"Uzasadnienie SÄ™dziego AI: {justification}\n")
                        f.write("="*80 + "\n") # Oddzielenie kolejnego modelu
                        
            elif result: # JeÅ›li nie ma sÄ™dziego, ale wynik jest OK
                 if output_file:
                    with open(output_file, 'a', encoding='utf-8') as f:
                        f.write("="*80 + "\n") # Oddzielenie kolejnego modelu
            
            if result:
                results.append(result)
            
            time.sleep(DEFAULT_SLEEP_BETWEEN_MODELS)
    
    generate_summary(results, output_file)
    print(f"\n{test_name_prefix} test zakoÅ„czony! Wyniki zapisane w: {output_file}")


def generate_summary(results, output_file):
    """Generuje podsumowanie wynikÃ³w testÃ³w"""
    if not results:
        return
    
    summary = f"\n{'='*100}\n"
    summary += "PODSUMOWANIE WYNIKÃ“W\n"
    summary += f"{'='*100}\n\n"
    
    models = list(set(r['model'] for r in results))
    
    summary += "Åšrednie czasy odpowiedzi i oceny jakoÅ›ci:\n"
    summary += "-" * 60 + "\n"
    
    for model in models:
        model_results = [r for r in results if r['model'] == model]
        if model_results:
            avg_first_token = sum(r['first_token_time'] for r in model_results) / len(model_results)
            avg_total_time = sum(r['total_time'] for r in model_results) / len(model_results)
            avg_length = sum(r['response_length'] for r in model_results) / len(model_results)
            
            # Oblicz Å›redniÄ… ocenÄ™ sÄ™dziego, jeÅ›li dostÄ™pne
            judge_ratings = [r['judge_rating'] for r in model_results if 'judge_rating' in r and r['judge_rating'] > 0]
            avg_judge_rating = sum(judge_ratings) / len(judge_ratings) if judge_ratings else "N/A"
            
            summary += f"{model}:\n"
            summary += f" Â - Åšredni czas pierwszego tokena: {avg_first_token:.2f}s\n"
            summary += f" Â - Åšredni caÅ‚kowity czas: {avg_total_time:.2f}s\n"
            summary += f" Â - Åšrednia dÅ‚ugoÅ›Ä‡ odpowiedzi: {avg_length:.0f} znakÃ³w\n"
            summary += f" Â - ZakoÅ„czonych testÃ³w: {len(model_results)}\n"
            summary += f" Â - Åšrednia ocena sÄ™dziego AI: {avg_judge_rating:.2f}/5\n" if isinstance(avg_judge_rating, float) else f" Â - Åšrednia ocena sÄ™dziego AI: {avg_judge_rating}\n"
            summary += "\n"
    
    summary += "ðŸ† RANKING SZYBKOÅšCI (pierwszy token - niÅ¼ej = lepiej):\n"
    summary += "-" * 60 + "\n"
    
    model_speed = {}
    for model in models:
        model_results = [r for r in results if r['model'] == model]
        if model_results:
            model_speed[model] = sum(r['first_token_time'] for r in model_results) / len(model_results)
    
    sorted_models_speed = sorted(model_speed.items(), key=lambda x: x[1])
    for i, (model, speed) in enumerate(sorted_models_speed, 1):
        medal = "ðŸ¥‡" if i == 1 else "ðŸ¥ˆ" if i == 2 else "ðŸ¥‰" if i == 3 else "  "
        summary += f"{medal} {i}. {model}: {speed:.2f}s\n"

    summary += f"\nðŸŽ–ï¸ RANKING JAKOÅšCI (ocena sÄ™dziego AI - wyÅ¼ej = lepiej):\n"
    summary += "-" * 60 + "\n"

    model_quality = {}
    for model in models:
        model_results = [r for r in results if r['model'] == model]
        if model_results:
            judge_ratings = [r['judge_rating'] for r in model_results if 'judge_rating' in r and r['judge_rating'] > 0]
            if judge_ratings:
                model_quality[model] = sum(judge_ratings) / len(judge_ratings)
    
    if model_quality:
        sorted_models_quality = sorted(model_quality.items(), key=lambda x: x[1], reverse=True)
        for i, (model, quality) in enumerate(sorted_models_quality, 1):
            summary += f"{i}. {model}: {quality:.2f}/5\n"
    else:
        summary += "Brak danych o ocenach sÄ™dziego AI (upewnij siÄ™, Å¼e klucz API Gemini jest poprawny).\n"
        
    print(summary)
    
    with open(output_file, 'a', encoding='utf-8') as f:
        f.write(summary)

if __name__ == "__main__":
    print("=== TESTER MODELI LLM ===")
    print("1. Kompleksowy test wszystkich modeli (10 rÃ³Å¼nych zadaÅ„, z ocenÄ… AI sÄ™dziego Gemini)")
    print("2. Szybki test (5 podstawowych zadaÅ„, z ocenÄ… AI sÄ™dziego Gemini)")
    print("3. WÅ‚asne pytanie do wszystkich modeli (bez oceny AI sÄ™dziego)")
    
    choice = input("\nWybierz opcjÄ™ (1, 2 lub 3): ").strip()
    
    if choice == "1":
        run_test_with_judge(test_type="comprehensive")
    elif choice == "2":
        run_test_with_judge(test_type="quick")
    elif choice == "3":
        prompt = input("Wpisz swoje pytanie: ").strip()
        if prompt:
            ask_all_models(prompt)
        else:
            print("Nie podano pytania!")
    else:
        print("NieprawidÅ‚owy wybÃ³r! Uruchamiam szybki test...")
        run_test_with_judge(test_type="quick")
