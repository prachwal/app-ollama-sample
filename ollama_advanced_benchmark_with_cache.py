import requests
import json
import time
import os
from datetime import datetime
import signal
from functools import reduce
from functools import reduce
import pickle
import csv
from pathlib import Path

# --- Konfiguracja (mo≈ºna przenie≈õƒá do osobnego pliku config.py/json) ---
OLLAMA_API_URL = "http://localhost:11434"
DEFAULT_TIMEOUT_PER_MODEL = 180 # Domy≈õlny timeout dla pojedynczej odpowiedzi modelu testowanego
DEFAULT_SLEEP_BETWEEN_MODELS = 2 # Domy≈õlna pauza miƒôdzy modelami testowanymi

# Konfiguracja dla modelu sƒôdziego (Gemini)
# Upewnij siƒô, ≈ºe masz klucz API do Gemini. Nie umieszczaj go tutaj na sta≈Çe!
# Zostanie poproszony o niego w trakcie dzia≈Çania skryptu.
GEMINI_API_URL = "https://generativelanguage.googleapis.com/v1beta/models"
GEMINI_JUDGE_MODEL_NAME = "gemini-2.5-flash" # Zaktualizowano na Gemini 2.5 Flash
GEMINI_JUDGE_TIMEOUT = 60 # Timeout dla odpowiedzi modelu sƒôdziego
# --- Koniec konfiguracji ---

# --- Konfiguracja cache ---
CACHE_DIR = Path("cache")
CACHE_FILE = CACHE_DIR / "models_metadata.pkl"
CACHE_EXPIRY_HOURS = 24  # Cache wa≈ºny przez 24 godziny

def get_available_models():
    """
    Pobiera listƒô dostƒôpnych modeli z Ollama wraz z podstawowymi metadanymi.
    Zwraca listƒô s≈Çownik√≥w, gdzie ka≈ºdy s≈Çownik reprezentuje model
    i zawiera klucze takie jak 'name', 'modified_at', 'size', 'digest'.
    """
    url = f"{OLLAMA_API_URL}/api/tags"
    try:
        response = requests.get(url)
        response.raise_for_status() # Wyrzuƒá wyjƒÖtek dla status√≥w 4xx/5xx
        data = response.json()
        # Zwracamy pe≈Çne s≈Çowniki modeli, a nie tylko ich nazwy
        return data.get('models', [])
    except requests.exceptions.ConnectionError:
        print(f"B≈ÇƒÖd po≈ÇƒÖczenia z Ollama na {OLLAMA_API_URL}. Upewnij siƒô, ≈ºe Ollama jest uruchomiona.")
        return []
    except requests.exceptions.RequestException as e:
        print(f"B≈ÇƒÖd pobierania modeli: {e}")
        return []

def get_all_models_with_detailed_metadata():
    """
    Pobiera listƒô wszystkich modeli z Ollama wraz ze szczeg√≥≈Çowymi metadanymi.
    Wykorzystuje endpointy /api/tags i /api/show.
    """
    basic_models_info = get_available_models()
    if not basic_models_info:
        print("Brak dostƒôpnych modeli Ollama do pobrania szczeg√≥≈Çowych metadanych.")
        return []

    detailed_models = []
    print("\nPobieram szczeg√≥≈Çowe metadane dla modeli Ollama (mo≈ºe chwilƒô potrwaƒá)...")
    for model_info in basic_models_info:
        model_name = model_info['name']
        url = f"{OLLAMA_API_URL}/api/show"
        payload = {"name": model_name}
        
        try:
            response = requests.post(url, json=payload, timeout=30) # Kr√≥tszy timeout dla show
            response.raise_for_status()
            detailed_data = response.json()
            
            # ≈ÅƒÖczymy podstawowe info z /api/tags z szczeg√≥≈Çami z /api/show
            combined_data = {**model_info, **detailed_data}
            detailed_models.append(combined_data)
            print(f" - Pobrane szczeg√≥≈Çy dla: {model_name}")
        except requests.exceptions.RequestException as e:
            print(f" - B≈ÇƒÖd pobierania szczeg√≥≈Ç√≥w dla {model_name}: {e}")
            # Dodajemy model z podstawowymi informacjami, je≈õli szczeg√≥≈Çy nie mog≈Çy byƒá pobrane
            detailed_models.append(model_info) 
        except Exception as e:
            print(f" - Nieoczekiwany b≈ÇƒÖd dla {model_name}: {e}")
            detailed_models.append(model_info)

    return detailed_models


def ask_ollama(model, prompt, test_name="", output_file=None, timeout=None, **model_options):
    """Zadaje pytanie do wybranego modelu z Ollama z timeoutem i dodatkowymi opcjami"""
    url = f"{OLLAMA_API_URL}/api/generate"
    
    options = {
        "temperature": 0.7,
        "top_k": 40,
        "top_p": 0.9,
        "num_predict": -1,
    }
    options.update(model_options)

    payload = {
        "model": model,
        "prompt": prompt,
        "options": options
    }
    
    current_timeout = timeout if timeout is not None else DEFAULT_TIMEOUT_PER_MODEL

    try:
        start_time = time.time()
        response = requests.post(url, json=payload, stream=True, timeout=current_timeout)
        response.raise_for_status()

        result_header = f"\n{'='*80}\n"
        result_header += f"Test: {test_name}\n"
        result_header += f"Model: {model}\n"
        result_header += f"Pytanie: {prompt}\n"
        result_header += "Odpowied≈∫: "
        
        print(result_header, end="", flush=True)
        
        first_token_time = None
        full_response = ""
        
        for line in response.iter_lines():
            if line:
                try:
                    data = json.loads(line.decode())
                    if 'response' in data:
                        if first_token_time is None:
                            first_token_time = time.time()
                        print(data['response'], end="", flush=True)
                        full_response += data['response']
                    
                    if data.get('done', False):
                        end_time = time.time()
                        total_time = end_time - start_time
                        first_token_delay = first_token_time - start_time if first_token_time else 0
                        
                        timing_info = f"\n\nCzas odpowiedzi:"
                        timing_info += f"\n ¬†- Pierwszy token: {first_token_delay:.2f}s"
                        timing_info += f"\n ¬†- Ca≈Çkowity czas: {total_time:.2f}s"
                        timing_info += f"\n ¬†- D≈Çugo≈õƒá odpowiedzi: {len(full_response)} znak√≥w\n"
                        
                        print(timing_info)
                        
                        # Zapisz do pliku je≈õli podano (tylko surowa odpowied≈∫ + timing)
                        if output_file:
                            with open(output_file, 'a', encoding='utf-8') as f:
                                f.write(result_header + full_response + timing_info + "\n")
                        
                        return {
                            'model': model,
                            'test_name': test_name,
                            'prompt': prompt,
                            'response': full_response,
                            'first_token_time': first_token_delay,
                            'total_time': total_time,
                            'response_length': len(full_response)
                        }
                        
                except json.JSONDecodeError:
                    continue
                
    except requests.exceptions.Timeout:
        error_msg = f"\n\nTIMEOUT: Model {model} przekroczy≈Ç limit {current_timeout}s."
        print(error_msg)
        if output_file:
            with open(output_file, 'a', encoding='utf-8') as f:
                f.write(f"\n{result_header}(Brak pe≈Çnej odpowiedzi z powodu timeoutu)\n{error_msg}\n")
        return None
    except requests.exceptions.RequestException as e:
        error_msg = f"\n\nB≈ÇƒÖd zapytania HTTP dla modelu {model}: {e}"
        print(error_msg)
        if output_file:
            with open(output_file, 'a', encoding='utf-8') as f:
                f.write(f"\n{result_header}(B≈ÇƒÖd po≈ÇƒÖczenia/zapytania)\n{error_msg}\n")
        return None
    except Exception as e:
        error_msg = f"\n\nNieoczekiwany b≈ÇƒÖd podczas pytania do modelu {model}: {e}"
        print(error_msg)
        if output_file:
            with open(output_file, 'a', encoding='utf-8') as f:
                f.write(f"\n{result_header}(Nieoczekiwany b≈ÇƒÖd)\n{error_msg}\n")
        return None

def judge_with_llm(model_response, original_prompt, gemini_api_key, judge_model_name=GEMINI_JUDGE_MODEL_NAME):
    """
    Wykorzystuje zewnƒôtrzny model LLM (Gemini) jako sƒôdziego do oceny jako≈õci odpowiedzi.
    Zwraca ocenƒô (int z 1-5) i uzasadnienie.
    """
    judge_prompt = f"""Oce≈Ñ jako≈õƒá poni≈ºszej odpowiedzi na oryginalne pytanie.
    Twoja ocena powinna dotyczyƒá:
    1. Poprawno≈õci (czy odpowied≈∫ jest prawdziwa/logiczna?).
    2. Kompletno≈õci (czy odpowied≈∫ w pe≈Çni odnosi siƒô do pytania?).
    3. Zrozumia≈Ço≈õci (czy odpowied≈∫ jest jasna i dobrze sformu≈Çowana?).
    4. Zgodno≈õci z instrukcjƒÖ (czy odpowied≈∫ spe≈Çnia wszystkie wymogi pytania, np. format kodu, komentarze?).

    Twoja ocena powinna byƒá w skali od 1 (bardzo s≈Çaba) do 5 (doskona≈Ça).
    Nastƒôpnie uzasadnij swojƒÖ ocenƒô w kilku zdaniach.

    Format odpowiedzi:
    OCENA: [liczba od 1 do 5]
    UZASADNIENIE: [Twoje uzasadnienie]

    ---
    ORYGINALNE PYTANIE:
    {original_prompt}

    ---
    ODPOWIED≈π DO OCENY:
    {model_response}
    """
    
    # URL dla API Gemini
    api_url = f"{GEMINI_API_URL}/{judge_model_name}:generateContent?key={gemini_api_key}"
    
    payload = {
        "contents": [
            {
                "role": "user",
                "parts": [{"text": judge_prompt}]
            }
        ],
        "generationConfig": {
            "temperature": 0.2, # Ni≈ºsza temperatura dla bardziej deterministycznych ocen
            "maxOutputTokens": 300 # Ogranicz d≈Çugo≈õƒá odpowiedzi sƒôdziego
        }
    }

    try:
        headers = {'Content-Type': 'application/json'}
        judge_response = requests.post(api_url, headers=headers, json=payload, timeout=GEMINI_JUDGE_TIMEOUT)
        judge_response.raise_for_status() # Sprawd≈∫ b≈Çƒôdy HTTP
        
        result_data = judge_response.json()
        
        # Parsowanie odpowiedzi Gemini
        if result_data and 'candidates' in result_data and len(result_data['candidates']) > 0:
            if 'content' in result_data['candidates'][0] and 'parts' in result_data['candidates'][0]['content'] and len(result_data['candidates'][0]['content']['parts']) > 0:
                judge_full_response = result_data['candidates'][0]['content']['parts'][0]['text']
            else:
                judge_full_response = "Brak tre≈õci odpowiedzi od sƒôdziego."
        else:
            judge_full_response = "Nieprawid≈Çowa struktura odpowiedzi od sƒôdziego."
        
        # Parsowanie oceny i uzasadnienia z tekstu odpowiedzi sƒôdziego
        rating = 0
        justification = "Brak uzasadnienia."
        
        lines = judge_full_response.split('\n')
        for line in lines:
            if line.startswith("OCENA:"):
                try:
                    rating = int(line.split(":")[1].strip())
                    if not (1 <= rating <= 5): # Upewnij siƒô, ≈ºe ocena jest w zakresie
                        rating = 0 
                except (ValueError, IndexError):
                    rating = 0 # Nie uda≈Ço siƒô sparsowaƒá
            elif line.startswith("UZASADNIENIE:"):
                justification = line.split(":", 1)[1].strip()
                
        return rating, justification
        
    except requests.exceptions.Timeout:
        print(f"\nB≈ÇƒÖd: Model sƒôdzia ({judge_model_name}) przekroczy≈Ç limit {GEMINI_JUDGE_TIMEOUT}s.")
        return 0, f"B≈ÇƒÖd sƒôdziego: Timeout ({GEMINI_JUDGE_TIMEOUT}s)"
    except requests.exceptions.RequestException as e:
        print(f"\nB≈ÇƒÖd zapytania HTTP do modelu sƒôdziego ({judge_model_name}): {e}")
        return 0, f"B≈ÇƒÖd sƒôdziego: B≈ÇƒÖd HTTP ({e})"
    except Exception as e:
        print(f"\nNieoczekiwany b≈ÇƒÖd w funkcji sƒôdziego: {e}")
        return 0, f"Nieoczekiwany b≈ÇƒÖd sƒôdziego: {e}"

def get_test_prompts():
    """Zwraca zestaw test√≥w do oceny modeli LLM z opcjami dla modeli"""
    return [
        {
            "name": "Przedstawienie",
            "prompt": "Przedstaw siƒô kr√≥tko - kim jeste≈õ i jakie masz mo≈ºliwo≈õci?",
            "options": {"temperature": 0.7, "num_predict": 500}
        },
        {
            "name": "Zadanie programistyczne - Python",
            "prompt": "Napisz funkcjƒô w Pythonie, kt√≥ra znajduje wszystkie liczby pierwsze mniejsze od n u≈ºywajƒÖc sita Eratostenesa. Dodaj komentarze i przyk≈Çad u≈ºycia.",
            "options": {"temperature": 0.3, "num_predict": 2000, "top_p": 0.95}
        },
        {
            "name": "Zadanie programistyczne - JavaScript",
            "prompt": "Napisz funkcjƒô JavaScript, kt√≥ra implementuje debounce z delay 300ms. Poka≈º przyk≈Çad u≈ºycia z obs≈ÇugƒÖ klikniƒôƒá przycisku.",
            "options": {"temperature": 0.3, "num_predict": 1500, "top_p": 0.95}
        },
        {
            "name": "Analiza sentymentu",
            "prompt": "Oce≈Ñ sentyment nastƒôpujƒÖcego tekstu na skali od -5 (bardzo negatywny) do +5 (bardzo pozytywny) i uzasadnij swojƒÖ ocenƒô:\n\n'Ten produkt to kompletna pora≈ºka! Nie dzia≈Ça≈Ç od pierwszego dnia, obs≈Çuga klienta ignoruje moje wiadomo≈õci, a zwrot pieniƒôdzy to koszmar. Zdecydowanie odradzam!'",
            "options": {"temperature": 0.5, "num_predict": 300}
        },
        {
            "name": "Logiczne rozumienie",
            "prompt": "RozwiƒÖ≈º zagadkƒô logicznƒÖ: Mam 3 pude≈Çka - czerwone, niebieskie i zielone. W ka≈ºdym jest jedna pi≈Çka: czerwona, niebieska lub zielona. Wiem, ≈ºe: 1) czerwona pi≈Çka nie jest w czerwonym pude≈Çku, 2) niebieska pi≈Çka nie jest w niebieskim pude≈Çku, 3) zielona pi≈Çka jest w czerwonym pude≈Çku. Gdzie jest ka≈ºda pi≈Çka?",
            "options": {"temperature": 0.1, "num_predict": 200}
        },
        {
            "name": "Streszczenie tekstu",
            "prompt": "Streƒá w 2-3 zdaniach nastƒôpujƒÖcy tekst:\n\n'Sztuczna inteligencja (AI) to dziedzina informatyki zajmujƒÖca siƒô tworzeniem system√≥w zdolnych do wykonywania zada≈Ñ wymagajƒÖcych ludzkiej inteligencji. Obejmuje to uczenie maszynowe, przetwarzanie jƒôzyka naturalnego, rozpoznawanie obraz√≥w i podejmowanie decyzji. AI ma szerokie zastosowania - od asystent√≥w g≈Çosowych, przez systemy rekomendacji, po autonomiczne pojazdy. Rozw√≥j AI niesie ogromne mo≈ºliwo≈õci, ale tak≈ºe wyzwania etyczne i spo≈Çeczne, kt√≥re wymagajƒÖ odpowiedzialnego podej≈õcia do implementacji tych technologii.'",
            "options": {"temperature": 0.6, "num_predict": 150}
        },
        {
            "name": "Kreatywne pisanie",
            "prompt": "Napisz kr√≥tkƒÖ historiƒô (3-4 akapity) o robocie, kt√≥ry po raz pierwszy do≈õwiadcza emocji. Historia powinna mieƒá beginning, middle i end.",
            "options": {"temperature": 0.8, "num_predict": 500}
        },
        {
            "name": "Analiza danych - SQL",
            "prompt": "Napisz zapytanie SQL, kt√≥re znajdzie top 5 klient√≥w wed≈Çug ≈ÇƒÖcznej warto≈õci zam√≥wie≈Ñ w ostatnim roku. Za≈Ç√≥≈º tabele: customers(id, name), orders(id, customer_id, order_date, total_amount).",
            "options": {"temperature": 0.3, "num_predict": 300}
        },
        {
            "name": "Matematyka",
            "prompt": "Wyja≈õnij krok po kroku, jak rozwiƒÖzaƒá r√≥wnanie kwadratowe: 2x¬≤ - 7x + 3 = 0",
            "options": {"temperature": 0.1, "num_predict": 400}
        },
        {
            "name": "T≈Çumaczenie i kontekst kulturowy",
            "prompt": "Przet≈Çumacz na angielski i wyja≈õnij kontekst kulturowy: 'Nie ma to jak u mamy' - polskie przys≈Çowie.",
            "options": {"temperature": 0.5, "num_predict": 300}
        }
    ]

def get_quick_test_prompts():
    """Zwraca skr√≥cony zestaw test√≥w do szybkiej oceny modeli z opcjami"""
    return [
        {
            "name": "Przedstawienie",
            "prompt": "Przedstaw siƒô kr√≥tko - kim jeste≈õ i jakie masz mo≈ºliwo≈õci?",
            "options": {"temperature": 0.7, "num_predict": 300}
        },
        {
            "name": "Programowanie - Python",
            "prompt": "Napisz prostƒÖ funkcjƒô Python, kt√≥ra sprawdza czy liczba jest parzysta.",
            "options": {"temperature": 0.3, "num_predict": 200}
        },
        {
            "name": "Analiza sentymentu",
            "prompt": "Oce≈Ñ sentyment tekstu od -5 do +5: 'Ten produkt to kompletna pora≈ºka!'",
            "options": {"temperature": 0.5, "num_predict": 100}
        },
        {
            "name": "Matematyka",
            "prompt": "RozwiƒÖ≈º: 2x + 5 = 13",
            "options": {"temperature": 0.1, "num_predict": 150}
        },
        {
            "name": "Logika",
            "prompt": "Je≈õli wszyscy ludzie sƒÖ ≈õmiertelni, a Sokrates jest cz≈Çowiekiem, to czy Sokrates jest ≈õmiertelny?",
            "options": {"temperature": 0.1, "num_predict": 100}
        }
    ]

def ask_all_models(prompt):
    """Zadaje to samo pytanie wszystkim dostƒôpnym modelom (funkcja kompatybilna wstecz)"""
    models = get_available_models()
    
    if not models:
        print("Nie znaleziono dostƒôpnych modeli.")
        return
    
    # WyciƒÖgamy tylko nazwy modeli do wy≈õwietlenia
    model_names = [model['name'] for model in models]
    print(f"Znaleziono {len(model_names)} modeli: {', '.join(model_names)}")
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"single_test_{timestamp}.txt"
    
    for model_info in models: # Iterujemy po pe≈Çnych s≈Çownikach modeli
        model_name = model_info['name']
        # Tutaj nie ma oceny sƒôdziego, bo to pojedyncze pytanie
        ask_ollama(model_name, prompt, "Pojedyncze pytanie", output_file)
        time.sleep(DEFAULT_SLEEP_BETWEEN_MODELS)


def run_test_with_judge(test_type="comprehensive"):
    """Uruchamia test wszystkich modeli z ocenƒÖ AI sƒôdziego."""
    models_info = get_available_models() # Pobieramy pe≈Çne info o modelach
    
    if not models_info:
        print("Nie znaleziono dostƒôpnych modeli.")
        return

    # WyciƒÖgamy tylko nazwy modeli do przekazania do test√≥w
    models = [model['name'] for model in models_info]

    gemini_api_key = os.environ.get("GEMINI_API_KEY")
    if not gemini_api_key:
        print("\n--- UWAGA: KLUCZ API GEMINI ---")
        print("Aby u≈ºyƒá Gemini jako sƒôdziego, potrzebujesz klucza API.")
        print("Mo≈ºesz go uzyskaƒá na: https://aistudio.google.com/app/apikey")
        gemini_api_key = input("Wprowad≈∫ sw√≥j klucz API Gemini: ").strip()
        if not gemini_api_key:
            print("Brak klucza API Gemini. Testy zostanƒÖ przeprowadzone BEZ oceny sƒôdziego AI.")
            use_judge = False
        else:
            use_judge = True
    else:
        print("U≈ºywam klucza API Gemini z zmiennej ≈õrodowiskowej GEMINI_API_KEY.")
        use_judge = True

    # Utw√≥rz plik wynik√≥w z timestampem
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    if test_type == "comprehensive":
        output_file = f"test_results_{timestamp}.txt"
        test_prompts = get_test_prompts()
        test_name_prefix = "Kompleksowy"
    else: # quick
        output_file = f"quick_test_{timestamp}.txt"
        test_prompts = get_quick_test_prompts()
        test_name_prefix = "Szybki"
    
    # Nag≈Ç√≥wek pliku
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(f"{test_name_prefix} test modeli LLM - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Testowane modele: {', '.join(models)}\n")
        f.write(f"Sƒôdzia AI: {GEMINI_JUDGE_MODEL_NAME} (status: {'aktywny' if use_judge else 'nieaktywny'})\n")
        f.write("="*100 + "\n\n")
    
    print(f"Rozpoczynam {test_name_prefix.lower()} test {len(models)} modeli z {len(test_prompts)} zadaniami...")
    print(f"Wyniki bƒôdƒÖ zapisywane do: {output_file}")
    
    results = []
    
    for i, test in enumerate(test_prompts, 1):
        print(f"\n{'#'*60}")
        print(f"TEST {i}/{len(test_prompts)}: {test['name']}")
        print(f"{'#'*60}")
        
        for model_name in models: # Iterujemy po nazwach modeli
            # U≈ºyj timeoutu z opcji promptu, lub domy≈õlnego
            timeout_for_task = test.get('options', {}).get('timeout', DEFAULT_TIMEOUT_PER_MODEL)
            result = ask_ollama(model_name, test['prompt'], test['name'], output_file, timeout=timeout_for_task, **test.get('options', {}))
            
            if result and use_judge: # Tylko je≈õli klucz API Gemini jest dostƒôpny
                print("\n--- Ocena sƒôdziego AI ---", end="", flush=True)
                rating, justification = judge_with_llm(result['response'], test['prompt'], gemini_api_key)
                result['judge_rating'] = rating
                result['judge_justification'] = justification
                
                print(f"\nOcena: {rating}/5")
                print(f"Uzasadnienie: {justification}")
                print("--------------------------\n")
                
                # Zaktualizuj plik wynik√≥w o ocenƒô sƒôdziego
                if output_file:
                    with open(output_file, 'a', encoding='utf-8') as f:
                        f.write(f"\nOcena Sƒôdziego AI ({GEMINI_JUDGE_MODEL_NAME}): {rating}/5\n")
                        f.write(f"Uzasadnienie Sƒôdziego AI: {justification}\n")
                        f.write("="*80 + "\n") # Oddzielenie kolejnego modelu
                        
            elif result: # Je≈õli nie ma sƒôdziego, ale wynik jest OK
                 if output_file:
                    with open(output_file, 'a', encoding='utf-8') as f:
                        f.write("="*80 + "\n") # Oddzielenie kolejnego modelu
            
            if result:
                results.append(result)
            
            time.sleep(DEFAULT_SLEEP_BETWEEN_MODELS)
    
    generate_summary(results, output_file)
    print(f"\n{test_name_prefix} test zako≈Ñczony! Wyniki zapisane w: {output_file}")


def generate_summary(results, output_file):
    """Generuje podsumowanie wynik√≥w test√≥w"""
    if not results:
        return
    
    summary = f"\n{'='*100}\n"
    summary += "PODSUMOWANIE WYNIK√ìW\n"
    summary += f"{'='*100}\n\n"
    
    models = list(set(r['model'] for r in results))
    
    summary += "≈örednie czasy odpowiedzi i oceny jako≈õci:\n"
    summary += "-" * 60 + "\n"
    
    for model in models:
        model_results = [r for r in results if r['model'] == model]
        if model_results:
            avg_first_token = sum(r['first_token_time'] for r in model_results) / len(model_results)
            avg_total_time = sum(r['total_time'] for r in model_results) / len(model_results)
            avg_length = sum(r['response_length'] for r in model_results) / len(model_results)
            
            # Oblicz ≈õredniƒÖ ocenƒô sƒôdziego, je≈õli dostƒôpne
            judge_ratings = [r['judge_rating'] for r in model_results if 'judge_rating' in r and r['judge_rating'] > 0]
            avg_judge_rating = sum(judge_ratings) / len(judge_ratings) if judge_ratings else "N/A"
            
            summary += f"{model}:\n"
            summary += f" ¬†- ≈öredni czas pierwszego tokena: {avg_first_token:.2f}s\n"
            summary += f" ¬†- ≈öredni ca≈Çkowity czas: {avg_total_time:.2f}s\n"
            summary += f" ¬†- ≈örednia d≈Çugo≈õƒá odpowiedzi: {avg_length:.0f} znak√≥w\n"
            summary += f" ¬†- Zako≈Ñczonych test√≥w: {len(model_results)}\n"
            summary += f" ¬†- ≈örednia ocena sƒôdziego AI: {avg_judge_rating:.2f}/5\n" if isinstance(avg_judge_rating, float) else f" ¬†- ≈örednia ocena sƒôdziego AI: {avg_judge_rating}\n"
            summary += "\n"
    
    summary += "Ranking szybko≈õci (pierwszy token - ni≈ºej = szybciej):\n"
    summary += "-" * 60 + "\n"
    
    model_speed = {}
    for model in models:
        model_results = [r for r in results if r['model'] == model]
        if model_results:
            model_speed[model] = sum(r['first_token_time'] for r in model_results) / len(model_results)
    
    sorted_models_speed = sorted(model_speed.items(), key=lambda x: x[1])
    for i, (model, speed) in enumerate(sorted_models_speed, 1):
        summary += f"{i}. {model}: {speed:.2f}s\n"

    summary += "\nRanking jako≈õci (ocena sƒôdziego AI - wy≈ºej = lepiej):\n"
    summary += "-" * 60 + "\n"

    model_quality = {}
    for model in models:
        model_results = [r for r in results if r['model'] == model]
        if model_results:
            judge_ratings = [r['judge_rating'] for r in model_results if 'judge_rating' in r and r['judge_rating'] > 0]
            if judge_ratings:
                model_quality[model] = sum(judge_ratings) / len(judge_ratings)
    
    if model_quality:
        sorted_models_quality = sorted(model_quality.items(), key=lambda x: x[1], reverse=True)
        for i, (model, quality) in enumerate(sorted_models_quality, 1):
            summary += f"{i}. {model}: {quality:.2f}/5\n"
    else:
        summary += "Brak danych o ocenach sƒôdziego AI (upewnij siƒô, ≈ºe klucz API Gemini jest poprawny).\n"
        
    print(summary)
    
    with open(output_file, 'a', encoding='utf-8') as f:
        f.write(summary)

def get_enhanced_model_metadata():
    """
    Pobiera rozszerzone metadane modeli z bardziej u≈ºytecznymi informacjami.
    Ekstrakcja kluczowych danych technicznych w czytelnej formie.
    """
    detailed_models = get_all_models_with_detailed_metadata()
    enhanced_models = []
    
    for model_data in detailed_models:
        # Podstawowe informacje
        enhanced_info = {
            'name': model_data.get('name', 'N/A'),
            'size_bytes': model_data.get('size', 0),
            'size_gb': round(model_data.get('size', 0) / (1024**3), 2),
            'modified_at': model_data.get('modified_at', 'N/A'),
            'digest': model_data.get('digest', 'N/A')[:12],  # Skr√≥cona wersja
        }
        
        # Szczeg√≥≈Çy z /api/tags
        details = model_data.get('details', {})
        enhanced_info.update({
            'family': details.get('family', 'N/A'),
            'parameter_size': details.get('parameter_size', 'N/A'),
            'quantization': details.get('quantization_level', 'N/A'),
            'format': details.get('format', 'N/A')
        })
        
        # Zaawansowane informacje z /api/show
        model_info = model_data.get('model_info', {})
        if model_info:
            # Architektura modelu
            enhanced_info.update({
                'architecture': model_info.get('general.architecture', 'N/A'),
                'context_length': model_info.get(f"{enhanced_info['family']}.context_length", 'N/A'),
                'embedding_length': model_info.get(f"{enhanced_info['family']}.embedding_length", 'N/A'),
                'layers_count': model_info.get(f"{enhanced_info['family']}.block_count", 'N/A'),
                'attention_heads': model_info.get(f"{enhanced_info['family']}.attention.head_count", 'N/A'),
                'parameter_count': model_info.get('general.parameter_count', 'N/A')
            })
            
            # Tokenizer info
            enhanced_info.update({
                'tokenizer_type': model_info.get('tokenizer.ggml.model', 'N/A'),
                'vocab_size': len(model_info.get('tokenizer.ggml.tokens', [])) if model_info.get('tokenizer.ggml.tokens') else 'N/A',
                'bos_token_id': model_info.get('tokenizer.ggml.bos_token_id', 'N/A'),
                'eos_token_id': model_info.get('tokenizer.ggml.eos_token_id', 'N/A')
            })
        
        # Capabilities
        enhanced_info['capabilities'] = model_data.get('capabilities', [])
        
        # Tensor statistics
        tensors = model_data.get('tensors', [])
        if tensors:
            enhanced_info.update({
                'tensor_count': len(tensors),
                'weight_types': list(set(t.get('type', 'Unknown') for t in tensors)),
                'total_parameters': sum(
                    reduce(lambda x, y: x * y, t.get('shape', [1]), 1) 
                    for t in tensors
                ) if tensors else 'N/A'
            })
        
        enhanced_models.append(enhanced_info)
    
    return enhanced_models

def is_cache_valid():
    """Sprawdza czy cache jest aktualny"""
    if not CACHE_FILE.exists():
        return False
    
    cache_time = datetime.fromtimestamp(CACHE_FILE.stat().st_mtime)
    current_time = datetime.now()
    age_hours = (current_time - cache_time).total_seconds() / 3600
    
    return age_hours < CACHE_EXPIRY_HOURS

def save_to_cache(data):
    """Zapisuje dane do cache"""
    try:
        CACHE_DIR.mkdir(exist_ok=True)
        with open(CACHE_FILE, 'wb') as f:
            pickle.dump(data, f)
        print(f"‚úÖ Dane zapisane do cache: {CACHE_FILE}")
    except Exception as e:
        print(f"‚ö†Ô∏è B≈ÇƒÖd zapisu do cache: {e}")

def load_from_cache():
    """≈Åaduje dane z cache"""
    try:
        with open(CACHE_FILE, 'rb') as f:
            data = pickle.load(f)
        print(f"‚úÖ Dane za≈Çadowane z cache: {CACHE_FILE}")
        return data
    except Exception as e:
        print(f"‚ö†Ô∏è B≈ÇƒÖd odczytu cache: {e}")
        return None

def get_all_models_with_detailed_metadata_cached(force_refresh=False):
    """
    Pobiera listƒô wszystkich modeli z cache lub API Ollama.
    
    Args:
        force_refresh (bool): Wymu≈õ od≈õwie≈ºenie danych z API
    """
    # Sprawd≈∫ cache je≈õli nie wymuszamy od≈õwie≈ºenia
    if not force_refresh and is_cache_valid():
        cached_data = load_from_cache()
        if cached_data:
            return cached_data
    
    print("üîÑ Pobieranie ≈õwie≈ºych danych z API Ollama...")
    
    # Pobierz ≈õwie≈ºe dane z API
    fresh_data = get_all_models_with_detailed_metadata()
    
    # Zapisz do cache
    if fresh_data:
        save_to_cache(fresh_data)
    
    return fresh_data

def export_models_metadata(models_data, format_type="json", filename=None):
    """
    Eksportuje metadane modeli do r√≥≈ºnych format√≥w.
    
    Args:
        models_data: Lista metadanych modeli
        format_type: 'json', 'csv', 'txt'
        filename: Nazwa pliku (opcjonalne)
    """
    if not models_data:
        print("‚ùå Brak danych do eksportu")
        return None
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    if filename is None:
        filename = f"models_metadata_{timestamp}"
    
    export_dir = Path("exports")
    export_dir.mkdir(exist_ok=True)
    
    try:
        if format_type.lower() == "json":
            filepath = export_dir / f"{filename}.json"
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(models_data, f, indent=2, ensure_ascii=False, default=str)
            
        elif format_type.lower() == "csv":
            filepath = export_dir / f"{filename}.csv"
            
            # Przygotuj dane dla CSV (flatten nested structures)
            csv_data = []
            for model in models_data:
                flat_model = {}
                for key, value in model.items():
                    if isinstance(value, dict):
                        for nested_key, nested_value in value.items():
                            flat_model[f"{key}_{nested_key}"] = nested_value
                    elif isinstance(value, list):
                        flat_model[key] = ", ".join(str(v) for v in value)
                    else:
                        flat_model[key] = value
                csv_data.append(flat_model)
            
            if csv_data:
                with open(filepath, 'w', newline='', encoding='utf-8') as f:
                    writer = csv.DictWriter(f, fieldnames=csv_data[0].keys())
                    writer.writeheader()
                    writer.writerows(csv_data)
            
        elif format_type.lower() == "txt":
            filepath = export_dir / f"{filename}.txt"
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write("RAPORT METADANYCH MODELI OLLAMA\n")
                f.write("=" * 50 + "\n")
                f.write(f"Data generowania: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"Liczba modeli: {len(models_data)}\n\n")
                
                for model in models_data:
                    f.write(f"MODEL: {model.get('name', 'N/A')}\n")
                    f.write("-" * 40 + "\n")
                    for key, value in model.items():
                        if key != 'name':
                            f.write(f"{key}: {value}\n")
                    f.write("\n")
        else:
            print(f"‚ùå Nieznany format: {format_type}")
            return None
        
        print(f"‚úÖ Eksport zako≈Ñczony: {filepath}")
        return filepath
        
    except Exception as e:
        print(f"‚ùå B≈ÇƒÖd eksportu: {e}")
        return None

def compare_models(models_data, criteria=['parameter_size', 'size_gb', 'context_length', 'quantization']):
    """
    Por√≥wnuje modele wed≈Çug wybranych kryteri√≥w.
    
    Args:
        models_data: Lista metadanych modeli  
        criteria: Lista kryteri√≥w do por√≥wnania
    """
    if not models_data:
        print("‚ùå Brak danych do por√≥wnania")
        return
    
    print("\n" + "="*80)
    print("üîç POR√ìWNANIE MODELI")
    print("="*80)
    
    # Przygotuj dane do por√≥wnania
    comparison_data = []
    for model in models_data:
        model_info = {'name': model.get('name', 'N/A')}
        for criterion in criteria:
            model_info[criterion] = model.get(criterion, 'N/A')
        comparison_data.append(model_info)
    
    # Sortuj wed≈Çug pierwszego kryterium
    if criteria and criteria[0] in ['parameter_size', 'size_gb', 'context_length']:
        try:
            comparison_data.sort(key=lambda x: float(str(x[criteria[0]]).replace('B', '').replace('GB', '')) 
                               if x[criteria[0]] != 'N/A' else 0, reverse=True)
        except:
            pass  # Je≈õli sortowanie nie wyjdzie, zostaw oryginalnƒÖ kolejno≈õƒá
    
    # Wy≈õwietl tabele por√≥wnawczƒÖ
    print(f"{'Model':<25}", end="")
    for criterion in criteria:
        print(f"{criterion:<15}", end="")
    print()
    print("-" * (25 + len(criteria) * 15))
    
    for model_info in comparison_data:
        print(f"{model_info['name']:<25}", end="")
        for criterion in criteria:
            value = str(model_info.get(criterion, 'N/A'))
            print(f"{value:<15}", end="")
        print()
    
    # Dodaj ranking dla ka≈ºdego kryterium
    print(f"\nüèÜ RANKINGI:")
    for criterion in criteria:
        print(f"\n{criterion.upper()}:")
        
        # Sortuj wed≈Çug aktualnego kryterium
        if criterion in ['size_gb', 'context_length']:
            try:
                sorted_models = sorted(comparison_data, 
                                     key=lambda x: float(str(x[criterion]).replace('GB', '')) 
                                     if x[criterion] != 'N/A' else 0, reverse=True)
            except:
                sorted_models = comparison_data
        elif 'parameter' in criterion:
            try:
                sorted_models = sorted(comparison_data,
                                     key=lambda x: float(str(x[criterion]).replace('B', '')) 
                                     if x[criterion] != 'N/A' else 0, reverse=True)
            except:
                sorted_models = comparison_data
        else:
            sorted_models = comparison_data
        
        for i, model_info in enumerate(sorted_models[:5], 1):  # Top 5
            medal = "ü•á" if i == 1 else "ü•à" if i == 2 else "ü•â" if i == 3 else "  "
            print(f"  {medal} {i}. {model_info['name']}: {model_info[criterion]}")

def display_enhanced_metadata():
    """Wy≈õwietla rozszerzone metadane w czytelnej formie"""
    enhanced_data = get_enhanced_model_metadata()
    
    if not enhanced_data:
        print("Brak danych o modelach.")
        return
    
    print("\n" + "="*100)
    print("ROZSZERZONE METADANE MODELI OLLAMA")
    print("="*100)
    
    for model in enhanced_data:
        print(f"\nü§ñ MODEL: {model['name']}")
        print("-" * 80)
        
        # Podstawowe informacje
        print(f"üìä PODSTAWOWE:")
        print(f"   Rozmiar: {model['size_gb']} GB ({model['size_bytes']:,} bajt√≥w)")
        print(f"   Rodzina: {model['family']}")
        print(f"   Parametry: {model['parameter_size']}")
        print(f"   Kwantyzacja: {model['quantization']}")
        print(f"   Format: {model['format']}")
        print(f"   Zmodyfikowano: {model['modified_at']}")
        
        # Architektura
        if model.get('architecture') != 'N/A':
            print(f"\nüèóÔ∏è ARCHITEKTURA:")
            print(f"   Typ: {model['architecture']}")
            print(f"   Warstwy: {model['layers_count']}")
            print(f"   Heads attention: {model['attention_heads']}")
            print(f"   D≈Çugo≈õƒá kontekstu: {model['context_length']}")
            print(f"   Embedding: {model['embedding_length']}")
            if model.get('parameter_count') != 'N/A':
                print(f"   Parametry (dok≈Çadne): {model['parameter_count']:,}")
        
        # Tokenizer
        if model.get('tokenizer_type') != 'N/A':
            print(f"\nüî§ TOKENIZER:")
            print(f"   Typ: {model['tokenizer_type']}")
            print(f"   BOS token ID: {model['bos_token_id']}")
            print(f"   EOS token ID: {model['eos_token_id']}")
        
        # Tensory
        if model.get('tensor_count'):
            print(f"\n‚öôÔ∏è TENSORY:")
            print(f"   Liczba tensor√≥w: {model['tensor_count']}")
            print(f"   Typy wag: {', '.join(model['weight_types'])}")
        
        # Mo≈ºliwo≈õci
        if model.get('capabilities'):
            print(f"\n‚ú® MO≈ªLIWO≈öCI: {', '.join(model['capabilities'])}")

def filter_similar_models(enhanced_data, reference_model_name="gemma2:2b"):
    """
    Filtruje modele o podobnych parametrach do modelu referencyjnego
    
    Args:
        enhanced_data: Lista modeli z rozszerzonymi metadanymi
        reference_model_name: Nazwa modelu referencyjnego (domy≈õlnie gemma2:2b)
    
    Returns:
        Lista modeli o podobnych parametrach
    """
    if not enhanced_data:
        print("‚ùå Brak danych do filtrowania")
        return []
    
    # Znajd≈∫ model referencyjny
    reference_model = None
    for model in enhanced_data:
        if model.get('name') == reference_model_name:
            reference_model = model
            break
    
    if not reference_model:
        print(f"‚ùå Nie znaleziono modelu referencyjnego: {reference_model_name}")
        print("Dostƒôpne modele:")
        for model in enhanced_data:
            print(f"  - {model.get('name', 'N/A')}")
        return []
    
    print(f"\nüéØ MODEL REFERENCYJNY: {reference_model_name}")
    print(f"   Parametry: {reference_model.get('parameter_size', 'N/A')}")
    print(f"   Rozmiar: {reference_model.get('size_gb', 'N/A')} GB")
    print(f"   Kontekst: {reference_model.get('context_length', 'N/A')}")
    print(f"   Warstwy: {reference_model.get('layers_count', 'N/A')}")
    print(f"   Kwantyzacja: {reference_model.get('quantization', 'N/A')}")
    print(f"   Architektura: {reference_model.get('architecture', 'N/A')}")
    
    # Kryteria filtrowania z tolerancjƒÖ
    ref_size_gb = reference_model.get('size_gb', 0)
    ref_context = reference_model.get('context_length', 0)
    ref_layers = reference_model.get('layers_count', 0)
    ref_param_count = reference_model.get('parameter_count', 0)
    
    # Konwertuj parameter_size na liczby dla por√≥wnania
    def extract_param_number(param_size):
        if isinstance(param_size, str) and param_size != 'N/A':
            # WyciƒÖgnij liczbƒô z tekstu typu "2.6B", "1.5B" itp.
            import re
            match = re.search(r'(\d+\.?\d*)', param_size)
            if match:
                return float(match.group(1))
        return 0
    
    ref_param_number = extract_param_number(reference_model.get('parameter_size', '0'))
    
    print(f"\nüîç KRYTERIA FILTROWANIA:")
    print(f"   Rozmiar pliku: {ref_size_gb-0.5:.1f} - {ref_size_gb+0.5:.1f} GB")
    print(f"   Parametry: {ref_param_number-1:.1f} - {ref_param_number+1:.1f}B")
    print(f"   Kontekst: {max(0, ref_context-10000)} - {ref_context+50000}")
    print(f"   Warstwy: {max(0, ref_layers-5)} - {ref_layers+5}")
    
    similar_models = []
    
    for model in enhanced_data:
        if model.get('name') == reference_model_name:
            continue  # Pomi≈Ñ model referencyjny
        
        model_size_gb = model.get('size_gb', 0)
        model_context = model.get('context_length', 0) if model.get('context_length') != 'N/A' else 0
        model_layers = model.get('layers_count', 0) if model.get('layers_count') != 'N/A' else 0
        model_param_number = extract_param_number(model.get('parameter_size', '0'))
        
        # Sprawd≈∫ kryteria podobie≈Ñstwa
        size_similar = abs(model_size_gb - ref_size_gb) <= 0.5  # ¬±0.5 GB
        param_similar = abs(model_param_number - ref_param_number) <= 1.0  # ¬±1B parametr√≥w
        context_similar = abs(model_context - ref_context) <= 50000 if ref_context > 0 else True  # ¬±50k kontekstu
        layers_similar = abs(model_layers - ref_layers) <= 5 if ref_layers > 0 else True  # ¬±5 warstw
        
        # Model jest podobny je≈õli spe≈Çnia przynajmniej 2 z 4 kryteri√≥w
        similarity_score = sum([size_similar, param_similar, context_similar, layers_similar])
        
        if similarity_score >= 2:
            model['similarity_score'] = similarity_score
            model['size_diff'] = abs(model_size_gb - ref_size_gb)
            model['param_diff'] = abs(model_param_number - ref_param_number)
            similar_models.append(model)
    
    # Sortuj wed≈Çug wyniku podobie≈Ñstwa (malejƒÖco) i r√≥≈ºnicy rozmiaru (rosnƒÖco)
    similar_models.sort(key=lambda x: (-x['similarity_score'], x['size_diff']))
    
    print(f"\nüìã ZNALEZIONO {len(similar_models)} PODOBNYCH MODELI:")
    print("-" * 80)
    
    for i, model in enumerate(similar_models, 1):
        print(f"\n{i}. {model.get('name', 'N/A')}")
        print(f"   üìä Wynik podobie≈Ñstwa: {model['similarity_score']}/4")
        print(f"   üìÅ Rozmiar: {model.get('size_gb', 'N/A')} GB (r√≥≈ºnica: {model['size_diff']:.2f} GB)")
        print(f"   üß† Parametry: {model.get('parameter_size', 'N/A')} (r√≥≈ºnica: {model['param_diff']:.1f}B)")
        print(f"   üìù Kontekst: {model.get('context_length', 'N/A')}")
        print(f"   üèóÔ∏è Warstwy: {model.get('layers_count', 'N/A')}")
        print(f"   ‚öôÔ∏è Kwantyzacja: {model.get('quantization', 'N/A')}")
    
    if not similar_models:
        print("‚ùå Nie znaleziono modeli o podobnych parametrach")
    
    return similar_models

def filter_similar_models_from_csv(csv_file_path, reference_model_name="gemma2:2b", export_results=False):
    """
    Filtruje modele z pliku CSV na podstawie podobie≈Ñstwa do modelu referencyjnego
    
    Args:
        csv_file_path: ≈öcie≈ºka do pliku CSV z metadanymi
        reference_model_name: Nazwa modelu referencyjnego
        export_results: Czy wyeksportowaƒá wyniki do pliku
        
    Returns:
        Lista przefiltrowanych modeli
    """
    try:
        import pandas as pd
        
        # Wczytaj CSV
        df = pd.read_csv(csv_file_path)
        print(f"üìÅ Wczytano {len(df)} modeli z pliku: {csv_file_path}")
        
        # Znajd≈∫ model referencyjny
        ref_model = df[df['name'] == reference_model_name]
        if ref_model.empty:
            print(f"‚ùå Nie znaleziono modelu referencyjnego: {reference_model_name}")
            print("Dostƒôpne modele:")
            for name in df['name'].tolist():
                print(f"  - {name}")
            return []
        
        ref_row = ref_model.iloc[0]
        print(f"\nüéØ MODEL REFERENCYJNY: {reference_model_name}")
        print(f"   Parametry: {ref_row.get('parameter_size', 'N/A')}")
        print(f"   Rozmiar: {ref_row.get('size_gb', 'N/A')} GB")
        print(f"   Kontekst: {ref_row.get('context_length', 'N/A')}")
        print(f"   Warstwy: {ref_row.get('layers_count', 'N/A')}")
        print(f"   Kwantyzacja: {ref_row.get('quantization', 'N/A')}")
        
        # Parametry referencyjne
        ref_size_gb = ref_row.get('size_gb', 0)
        ref_context = ref_row.get('context_length', 0)
        ref_layers = ref_row.get('layers_count', 0)
        
        def extract_param_number(param_size):
            if pd.isna(param_size) or param_size == 'N/A':
                return 0
            import re
            match = re.search(r'(\d+\.?\d*)', str(param_size))
            return float(match.group(1)) if match else 0
        
        ref_param_number = extract_param_number(ref_row.get('parameter_size', '0'))
        
        # Filtrowanie
        similar_models = []
        for idx, row in df.iterrows():
            if row['name'] == reference_model_name:
                continue
            
            model_size_gb = row.get('size_gb', 0)
            model_context = row.get('context_length', 0) if pd.notna(row.get('context_length')) else 0
            model_layers = row.get('layers_count', 0) if pd.notna(row.get('layers_count')) else 0
            model_param_number = extract_param_number(row.get('parameter_size', '0'))
            
            # Kryteria podobie≈Ñstwa
            size_similar = abs(model_size_gb - ref_size_gb) <= 0.5
            param_similar = abs(model_param_number - ref_param_number) <= 1.0
            context_similar = abs(model_context - ref_context) <= 50000 if ref_context > 0 else True
            layers_similar = abs(model_layers - ref_layers) <= 5 if ref_layers > 0 else True
            
            similarity_score = sum([size_similar, param_similar, context_similar, layers_similar])
            
            if similarity_score >= 2:
                model_dict = row.to_dict()
                model_dict['similarity_score'] = similarity_score
                model_dict['size_diff'] = abs(model_size_gb - ref_size_gb)
                model_dict['param_diff'] = abs(model_param_number - ref_param_number)
                similar_models.append(model_dict)
        
        # Sortowanie
        similar_models.sort(key=lambda x: (-x['similarity_score'], x['size_diff']))
        
        print(f"\nüìã ZNALEZIONO {len(similar_models)} PODOBNYCH MODELI:")
        print("-" * 80)
        
        for i, model in enumerate(similar_models, 1):
            print(f"\n{i}. {model.get('name', 'N/A')}")
            print(f"   üìä Wynik podobie≈Ñstwa: {model['similarity_score']}/4")
            print(f"   üìÅ Rozmiar: {model.get('size_gb', 'N/A')} GB (r√≥≈ºnica: {model['size_diff']:.2f} GB)")
            print(f"   üß† Parametry: {model.get('parameter_size', 'N/A')} (r√≥≈ºnica: {model['param_diff']:.1f}B)")
            print(f"   üìù Kontekst: {model.get('context_length', 'N/A')}")
            print(f"   üèóÔ∏è Warstwy: {model.get('layers_count', 'N/A')}")
            print(f"   ‚öôÔ∏è Kwantyzacja: {model.get('quantization', 'N/A')}")
        
        # Eksport wynik√≥w
        if export_results and similar_models:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            export_dir = Path("exports")
            export_dir.mkdir(exist_ok=True)
            
            # Eksport do CSV
            filtered_df = pd.DataFrame(similar_models)
            csv_filename = f"filtered_models_similar_to_{reference_model_name.replace(':', '_')}_{timestamp}.csv"
            csv_path = export_dir / csv_filename
            filtered_df.to_csv(csv_path, index=False)
            print(f"\nüíæ Przefiltrowane wyniki wyeksportowane do: {csv_path}")
            
            # Eksport do JSON
            json_filename = f"filtered_models_similar_to_{reference_model_name.replace(':', '_')}_{timestamp}.json"
            json_path = export_dir / json_filename
            filtered_df.to_json(json_path, orient='records', indent=2)
            print(f"üíæ Przefiltrowane wyniki wyeksportowane do: {json_path}")
        
        return similar_models
        
    except ImportError:
        print("‚ùå Brak modu≈Çu pandas. Zainstaluj: pip install pandas")
        return []
    except Exception as e:
        print(f"‚ùå B≈ÇƒÖd podczas filtrowania z CSV: {e}")
        return []

def interactive_csv_filter():
    """Interaktywne filtrowanie modeli z pliku CSV"""
    print("\nüìÑ FILTROWANIE Z PLIKU CSV")
    print("Dostƒôpne pliki CSV w folderze exports:")
    
    export_dir = Path("exports")
    if not export_dir.exists():
        print("‚ùå Folder exports nie istnieje")
        return
    
    csv_files = list(export_dir.glob("*.csv"))
    if not csv_files:
        print("‚ùå Brak plik√≥w CSV w folderze exports")
        return
    
    for i, csv_file in enumerate(csv_files, 1):
        print(f"{i}. {csv_file.name}")
    
    try:
        file_choice = int(input("\nWybierz numer pliku CSV: ").strip())
        if 1 <= file_choice <= len(csv_files):
            chosen_file = csv_files[file_choice - 1]
            
            # Wyb√≥r modelu referencyjnego
            print("\n1. U≈ºyj gemma2:2b jako model referencyjny")
            print("2. Wybierz inny model z pliku")
            
            ref_choice = input("Wybierz opcjƒô (1-2): ").strip()
            
            if ref_choice == "1":
                reference_model = "gemma2:2b"
            elif ref_choice == "2":
                # Wczytaj nazwy modeli z CSV
                try:
                    import pandas as pd
                    df = pd.read_csv(chosen_file)
                    print("\nDostƒôpne modele w pliku:")
                    for i, name in enumerate(df['name'].tolist(), 1):
                        print(f"{i}. {name}")
                    
                    model_choice = int(input("\nWybierz numer modelu referencyjnego: ").strip())
                    if 1 <= model_choice <= len(df):
                        reference_model = df['name'].iloc[model_choice - 1]
                    else:
                        print("‚ùå Nieprawid≈Çowy numer modelu")
                        return
                except ImportError:
                    print("‚ùå Brak modu≈Çu pandas")
                    return
                except Exception as e:
                    print(f"‚ùå B≈ÇƒÖd: {e}")
                    return
            else:
                print("‚ùå Nieprawid≈Çowy wyb√≥r")
                return
            
            # Pytanie o eksport
            export_choice = input("\nCzy wyeksportowaƒá wyniki? (t/n): ").strip().lower()
            export_results = export_choice in ['t', 'tak', 'y', 'yes']
            
            # Uruchom filtrowanie
            filter_similar_models_from_csv(chosen_file, reference_model, export_results)
            
        else:
            print("‚ùå Nieprawid≈Çowy numer pliku")
    except ValueError:
        print("‚ùå Wprowad≈∫ prawid≈Çowy numer")

def display_enhanced_metadata_with_options():
    """Wy≈õwietla rozszerzone metadane z opcjami cache, eksportu i por√≥wnania"""
    print("\nüìä OPCJE METADANYCH:")
    print("1. U≈ºyj cache (je≈õli dostƒôpny)")
    print("2. Od≈õwie≈º dane z API")
    print("3. Por√≥wnaj modele")
    print("4. Eksportuj dane")
    print("5. Filtruj modele podobne do gemma2:2b")
    print("6. Filtruj modele podobne do w≈Çasnego modelu")
    print("7. Filtruj modele z pliku CSV")
    
    choice = input("\nWybierz opcjƒô (1-7): ").strip()
    
    if choice == "1":
        enhanced_data = get_enhanced_model_metadata_cached()
        display_enhanced_metadata_from_data(enhanced_data)
    elif choice == "2":
        enhanced_data = get_enhanced_model_metadata_cached(force_refresh=True)
        display_enhanced_metadata_from_data(enhanced_data)
    elif choice == "3":
        enhanced_data = get_enhanced_model_metadata_cached()
        if enhanced_data:
            compare_models(enhanced_data)
        else:
            print("‚ùå Brak danych do por√≥wnania")
    elif choice == "4":
        enhanced_data = get_enhanced_model_metadata_cached()
        if enhanced_data:
            print("\nFormaty eksportu:")
            print("1. JSON")
            print("2. CSV") 
            print("3. TXT")
            
            format_choice = input("Wybierz format (1-3): ").strip()
            format_map = {"1": "json", "2": "csv", "3": "txt"}
            
            if format_choice in format_map:
                export_models_metadata(enhanced_data, format_map[format_choice])
            else:
                print("‚ùå Nieprawid≈Çowy wyb√≥r formatu")
        else:
            print("‚ùå Brak danych do eksportu")
    elif choice == "5":
        enhanced_data = get_enhanced_model_metadata_cached()
        if enhanced_data:
            filter_similar_models(enhanced_data, "gemma2:2b")
        else:
            print("‚ùå Brak danych do filtrowania")
    elif choice == "6":
        enhanced_data = get_enhanced_model_metadata_cached()
        if enhanced_data:
            print("\nDostƒôpne modele:")
            for i, model in enumerate(enhanced_data, 1):
                print(f"{i}. {model.get('name', 'N/A')}")
            
            try:
                model_choice = int(input("\nWybierz numer modelu referencyjnego: ").strip())
                if 1 <= model_choice <= len(enhanced_data):
                    ref_model_name = enhanced_data[model_choice - 1].get('name')
                    filter_similar_models(enhanced_data, ref_model_name)
                else:
                    print("‚ùå Nieprawid≈Çowy numer modelu")
            except ValueError:
                print("‚ùå Wprowad≈∫ prawid≈Çowy numer")
        else:
            print("‚ùå Brak danych do filtrowania")
    elif choice == "7":
        interactive_csv_filter()
    else:
        print("‚ùå Nieprawid≈Çowy wyb√≥r")

def get_enhanced_model_metadata_cached(force_refresh=False):
    """Wersja cached dla enhanced metadata"""
    detailed_models = get_all_models_with_detailed_metadata_cached(force_refresh)
    
    if not detailed_models:
        return []
    
    enhanced_models = []
    
    for model_data in detailed_models:
        # Podstawowe informacje
        enhanced_info = {
            'name': model_data.get('name', 'N/A'),
            'size_bytes': model_data.get('size', 0),
            'size_gb': round(model_data.get('size', 0) / (1024**3), 2),
            'modified_at': model_data.get('modified_at', 'N/A'),
            'digest': model_data.get('digest', 'N/A')[:12],  # Skr√≥cona wersja
        }
        
        # Szczeg√≥≈Çy z /api/tags
        details = model_data.get('details', {})
        enhanced_info.update({
            'family': details.get('family', 'N/A'),
            'parameter_size': details.get('parameter_size', 'N/A'),
            'quantization': details.get('quantization_level', 'N/A'),
            'format': details.get('format', 'N/A')
        })
        
        # Zaawansowane informacje z /api/show
        model_info = model_data.get('model_info', {})
        if model_info:
            # Architektura modelu
            enhanced_info.update({
                'architecture': model_info.get('general.architecture', 'N/A'),
                'context_length': model_info.get(f"{enhanced_info['family']}.context_length", 'N/A'),
                'embedding_length': model_info.get(f"{enhanced_info['family']}.embedding_length", 'N/A'),
                'layers_count': model_info.get(f"{enhanced_info['family']}.block_count", 'N/A'),
                'attention_heads': model_info.get(f"{enhanced_info['family']}.attention.head_count", 'N/A'),
                'parameter_count': model_info.get('general.parameter_count', 'N/A')
            })
            
            # Tokenizer info
            enhanced_info.update({
                'tokenizer_type': model_info.get('tokenizer.ggml.model', 'N/A'),
                'vocab_size': len(model_info.get('tokenizer.ggml.tokens', [])) if model_info.get('tokenizer.ggml.tokens') else 'N/A',
                'bos_token_id': model_info.get('tokenizer.ggml.bos_token_id', 'N/A'),
                'eos_token_id': model_info.get('tokenizer.ggml.eos_token_id', 'N/A')
            })
        
        # Capabilities
        enhanced_info['capabilities'] = model_data.get('capabilities', [])
        
        # Tensor statistics
        tensors = model_data.get('tensors', [])
        if tensors:
            enhanced_info.update({
                'tensor_count': len(tensors),
                'weight_types': list(set(t.get('type', 'Unknown') for t in tensors)),
                'total_parameters': sum(
                    reduce(lambda x, y: x * y, t.get('shape', [1]), 1) 
                    for t in tensors
                ) if tensors else 'N/A'
            })
        
        enhanced_models.append(enhanced_info)
    
    return enhanced_models

def display_enhanced_metadata_from_data(enhanced_data):
    """Wy≈õwietla enhanced metadata z przekazanych danych"""
    if not enhanced_data:
        print("Brak danych o modelach.")
        return
    
    print("\n" + "="*100)
    print("ROZSZERZONE METADANE MODELI OLLAMA")
    print("="*100)
    
    for model in enhanced_data:
        print(f"\nü§ñ MODEL: {model['name']}")
        print("-" * 80)
        
        # Podstawowe informacje
        print(f"üìä PODSTAWOWE:")
        print(f"   Rozmiar: {model['size_gb']} GB ({model['size_bytes']:,} bajt√≥w)")
        print(f"   Rodzina: {model['family']}")
        print(f"   Parametry: {model['parameter_size']}")
        print(f"   Kwantyzacja: {model['quantization']}")
        print(f"   Format: {model['format']}")
        print(f"   Zmodyfikowano: {model['modified_at']}")
        
        # Architektura
        if model.get('architecture') != 'N/A':
            print(f"\nüèóÔ∏è ARCHITEKTURA:")
            print(f"   Typ: {model['architecture']}")
            print(f"   Warstwy: {model['layers_count']}")
            print(f"   Heads attention: {model['attention_heads']}")
            print(f"   D≈Çugo≈õƒá kontekstu: {model['context_length']}")
            print(f"   Embedding: {model['embedding_length']}")
            if model.get('parameter_count') != 'N/A':
                print(f"   Parametry (dok≈Çadne): {model['parameter_count']:,}")
        
        # Tokenizer
        if model.get('tokenizer_type') != 'N/A':
            print(f"\nüî§ TOKENIZER:")
            print(f"   Typ: {model['tokenizer_type']}")
            print(f"   BOS token ID: {model['bos_token_id']}")
            print(f"   EOS token ID: {model['eos_token_id']}")
        
        # Tensory
        if model.get('tensor_count'):
            print(f"\n‚öôÔ∏è TENSORY:")
            print(f"   Liczba tensor√≥w: {model['tensor_count']}")
            print(f"   Typy wag: {', '.join(model['weight_types'])}")
        
        # Mo≈ºliwo≈õci
        if model.get('capabilities'):
            print(f"\n‚ú® MO≈ªLIWO≈öCI: {', '.join(model['capabilities'])}")

# --- Kod g≈Ç√≥wny ---
if __name__ == "__main__":
    print("=== TESTER MODELI LLM ===")
    print("1. Kompleksowy test wszystkich modeli (10 r√≥≈ºnych zada≈Ñ, z ocenƒÖ AI sƒôdziego Gemini)")
    print("2. Szybki test (5 podstawowych zada≈Ñ, z ocenƒÖ AI sƒôdziego Gemini)")
    print("3. W≈Çasne pytanie do wszystkich modeli (bez oceny AI sƒôdziego)")
    print("4. Wy≈õwietl szczeg√≥≈Çowe metadane wszystkich modeli Ollama")
    print("5. Wy≈õwietl rozszerzone metadane wszystkich modeli Ollama")
    
    choice = input("\nWybierz opcjƒô (1, 2, 3, 4 lub 5): ").strip()
    
    if choice == "1":
        run_test_with_judge(test_type="comprehensive")
    elif choice == "2":
        run_test_with_judge(test_type="quick")
    elif choice == "3":
        prompt = input("Wpisz swoje pytanie: ").strip()
        if prompt:
            ask_all_models(prompt)
        else:
            print("Nie podano pytania!")
    elif choice == "4":
        detailed_metadata = get_all_models_with_detailed_metadata()
        if detailed_metadata:
            print("\n--- SZCZEG√ì≈ÅOWE METADANE MODELI OLLAMA ---")
            for model_data in detailed_metadata:
                print(f"\nModel: {model_data.get('name', 'N/A')}")
                print(f"  Digest: {model_data.get('digest', 'N/A')}")
                print(f"  Rozmiar: {model_data.get('size', 'N/A')} bajt√≥w")
                print(f"  Zmodyfikowano: {model_data.get('modified_at', 'N/A')}")
                
                details = model_data.get('details', {})
                print(f"  Rodzina: {details.get('family', 'N/A')}")
                print(f"  Rozmiar parametr√≥w: {details.get('parameter_size', 'N/A')}")
                print(f"  Poziom kwantyzacji: {details.get('quantization_level', 'N/A')}")
                print(f"  Typ modelu: {details.get('model_type', 'N/A')}")
                print(f"  Format: {details.get('format', 'N/A')}")
                
                # Mo≈ºesz dodaƒá wiƒôcej p√≥l z 'details' lub innych kluczy, je≈õli sƒÖ interesujƒÖce
                # np. print(f"  Modelfile: {model_data.get('modelfile', 'N/A')[:100]}...") # Skr√≥cona wersja
        else:
            print("Nie uda≈Ço siƒô pobraƒá szczeg√≥≈Çowych metadanych modeli Ollama.")
    elif choice == "5":
        display_enhanced_metadata_with_options()
    else:
        print("Nieprawid≈Çowy wyb√≥r! Uruchamiam szybki test...")
        run_test_with_judge(test_type="quick")
